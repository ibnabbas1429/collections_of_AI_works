{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2d0ea0",
   "metadata": {},
   "source": [
    "# import the necesary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daedd5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c67b2636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8def2f01",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 20-21: truncated \\UXXXXXXXX escape (684341675.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"sys.path.append(r'C:\\Users\\work\\collections_of_AI_works\\Automated-MCQ-Generator-Using-Langchain-GEMINI-API-main\\src')\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m (unicode error) 'unicodeescape' codec can't decode bytes in position 20-21: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "\"\"\"sys.path.append(r'C:\\Users\\work\\collections_of_AI_works\\Automated-MCQ-Generator-Using-Langchain-GEMINI-API-main\\src')\n",
    "\n",
    "# Now you can import from the mcqgenerator package\n",
    "from mcqgenerator import logger\n",
    "\n",
    "# Now you can import from the mcqgenerator package\n",
    "from mcqgenerator import logger  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54f268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef0a9dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load environment variables from .env file\n",
    "load_dotenv()\n",
    "langsmith_tracing = os.getenv(\"LANGSMITH_TRACING\")\n",
    "langsmith_chain = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "#logging.info(\"Loading environment variables from .env file\")\n",
    "key = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1547b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37333380",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bf6e2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001A9A4426360>, default_metadata=(), model_kwargs={})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61976966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca6030a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_JSON = {\n",
    "    \"1\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1da4ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE=\"\"\"\n",
    "Text:{text}\n",
    "You are an expert MCQ maker. Given the above text, it is your job to \\\n",
    "create a quiz  of {number} multiple choice questions for {subject} students in {tone} tone. \n",
    "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
    "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. \\\n",
    "Ensure to make {number} MCQs\n",
    "### RESPONSE_JSON\n",
    "{response_json}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d657fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generation_prompt = PromptTemplate(\n",
    "    input_variables = [\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "    template= TEMPLATE \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c46aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chain = LLMChain(llm=llm, prompts=test_generation_prompt, output_key=\"quiz\", verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
